{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPn2ToNXtjvozMW2jTgU0w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaab/AI---Deep-Learning-com-PyTorch-e-Python/blob/main/Project_2_brest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROJECTO 2 CLASSIFICAÇÃO BINÁRIA BREST CANCER COM VALIDAÇÃO CRUZADA DROPOUT"
      ],
      "metadata": {
        "id": "KRKkkks9_yfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Etapa 1: Importação das bibliotecas"
      ],
      "metadata": {
        "id": "RFHVXCE0EByq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhfAKOMJDm93"
      },
      "outputs": [],
      "source": [
        "#!pip install skorch  #instalação da biblioteca skorch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from skorch import NeuralNetBinaryClassifier\n",
        "import torch\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import cross_val_score\n",
        "torch.__version__\n",
        "#!pip install torch==1.11.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6ARNVF0rEy6F",
        "outputId": "023f53cd-9ce6-4aa4-b239-339b11c891bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.11.0+cu102'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Etapa 2 : Base de Dados\n",
        "\n",
        "UPLOAD TO FILES entradas_breast.csv AND saidas_breast.csv FROM YOUR PC"
      ],
      "metadata": {
        "id": "ZvviC6-YGTQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8nXGCO3GYpH",
        "outputId": "6a1e28bc-05b7-43a1-d040-1e21616007da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d1d697d1a50>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsores = pd.read_csv('entradas_breast.csv')\n",
        "classe = pd.read_csv('saidas_breast.csv')"
      ],
      "metadata": {
        "id": "OFVREUfSGtIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(classe['0']);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "x3qUkY0gHo6k",
        "outputId": "b057e16b-f2d0-4280-fe86-5bc2b7d440f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classe.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbrc7gxDJ2DV",
        "outputId": "4d4b0284-15aa-49d1-b2ca-ef19685f58ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsores = np.array(previsores, dtype='float32')\n",
        "classe = np.array(classe, dtype='float32').squeeze(1)"
      ],
      "metadata": {
        "id": "i59rm6IpJPXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classe.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV4jmaWiKDc2",
        "outputId": "1929829f-2924-40ed-eb6a-79b0d808deb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(classe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AheKPckkKQk2",
        "outputId": "f18a63a5-fb0b-4afe-fc19-819cb0eee6dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(previsores)"
      ],
      "metadata": {
        "id": "XMD-6CL_HYmA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5f401e2-313a-4040-8901-ac7d64b9835d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Etapa 3: Classe para estrutura da rede neural"
      ],
      "metadata": {
        "id": "4pfezcFyHOiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class classificador_torch(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # 30 -> 16 -> 16 -> 1\n",
        "    self.dense0 = nn.Linear(30,16)\n",
        "    torch.nn.init.uniform_(self.dense0.weight)\n",
        "    self.activation0 = nn.ReLU()\n",
        "\n",
        "    self.dense1 = nn.Linear(16,16)\n",
        "    torch.nn.init.uniform_(self.dense1.weight)\n",
        "    self.activation1 = nn.ReLU()\n",
        "\n",
        "    self.dense2 = nn.Linear(16,1)\n",
        "    torch.nn.init.uniform_(self.dense2.weight)\n",
        "    self.activation2 = nn.ReLU()\n",
        "\n",
        "    self.output = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.dense0(x)\n",
        "    x = self.activation0(x)\n",
        "    x = self.dense1(x)\n",
        "    x = self.activation1(x)\n",
        "    x = self.dense2(x)\n",
        "    x = self.activation2(x)\n",
        "    x = self.output(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "MTpCGFofKfq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Etapa 4: Skorch"
      ],
      "metadata": {
        "id": "yF1s3LV2Kjp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classificador_sklearn = NeuralNetBinaryClassifier(\n",
        "    module=classificador_torch,\n",
        "    criterion=torch.nn.BCELoss,\n",
        "    optimizer =torch.optim.Adam,\n",
        "    lr=0.001,\n",
        "    optimizer__weight_decay=0.0001,\n",
        "    max_epochs=100,\n",
        "    batch_size=10,\n",
        "    train_split=False\n",
        "    )"
      ],
      "metadata": {
        "id": "AF7-yAI-KqAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Etapa 5 : Validação Cruzada"
      ],
      "metadata": {
        "id": "hcUNizhPPMAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resultados= cross_val_score(classificador_sklearn, previsores, classe, cv=10 , scoring= 'accuracy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ydy6gT7sP5Te",
        "outputId": "ab0a9bd7-577b-407c-8552-c7177f3bd47f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1094\u001b[0m  0.0654\n",
            "      2       37.1094  0.0634\n",
            "      3       37.1094  0.0711\n",
            "      4       37.1094  0.0675\n",
            "      5       37.1094  0.0624\n",
            "      6       37.1094  0.0814\n",
            "      7       37.1094  0.0682\n",
            "      8       37.1094  0.0606\n",
            "      9       37.1094  0.0701\n",
            "     10       37.1094  0.0670\n",
            "     11       37.1094  0.0642\n",
            "     12       37.1094  0.0683\n",
            "     13       37.1094  0.0768\n",
            "     14       37.1094  0.0673\n",
            "     15       37.1094  0.0631\n",
            "     16       37.1094  0.0719\n",
            "     17       37.1094  0.0668\n",
            "     18       37.1094  0.0711\n",
            "     19       37.1094  0.0675\n",
            "     20       37.1094  0.0711\n",
            "     21       37.1094  0.0764\n",
            "     22       37.1094  0.0755\n",
            "     23       37.1094  0.0764\n",
            "     24       37.1094  0.0751\n",
            "     25       37.1094  0.0665\n",
            "     26       37.1094  0.0844\n",
            "     27       37.1094  0.0735\n",
            "     28       37.1094  0.0674\n",
            "     29       37.1094  0.0659\n",
            "     30       37.1094  0.0727\n",
            "     31       37.1094  0.1007\n",
            "     32       37.1094  0.1049\n",
            "     33       37.1094  0.1106\n",
            "     34       37.1094  0.0964\n",
            "     35       37.1094  0.0889\n",
            "     36       37.1094  0.0921\n",
            "     37       37.1094  0.0901\n",
            "     38       37.1094  0.0963\n",
            "     39       37.1094  0.1045\n",
            "     40       37.1094  0.0930\n",
            "     41       37.1094  0.1041\n",
            "     42       37.1094  0.0856\n",
            "     43       37.1094  0.0940\n",
            "     44       37.1094  0.1019\n",
            "     45       37.1094  0.0866\n",
            "     46       37.1094  0.1028\n",
            "     47       37.1094  0.0918\n",
            "     48       37.1094  0.0909\n",
            "     49       37.1094  0.0970\n",
            "     50       37.1094  0.1239\n",
            "     51       37.1094  0.1069\n",
            "     52       37.1094  0.1002\n",
            "     53       37.1094  0.1219\n",
            "     54       37.1094  0.0947\n",
            "     55       37.1094  0.1124\n",
            "     56       37.1094  0.1198\n",
            "     57       37.1094  0.0962\n",
            "     58       37.1094  0.0952\n",
            "     59       37.1094  0.1314\n",
            "     60       37.1094  0.1146\n",
            "     61       37.1094  0.1122\n",
            "     62       37.1094  0.0819\n",
            "     63       37.1094  0.0730\n",
            "     64       37.1094  0.0637\n",
            "     65       37.1094  0.0659\n",
            "     66       37.1094  0.0678\n",
            "     67       37.1094  0.0667\n",
            "     68       37.1094  0.0624\n",
            "     69       37.1094  0.0727\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.1094\u001b[0m  0.0719\n",
            "      2       37.1094  0.0665\n",
            "      3       37.1094  0.0921\n",
            "      4       37.1094  0.0699\n",
            "      5       37.1094  0.0726\n",
            "      6       37.1094  0.0699\n",
            "      7       37.1094  0.0889\n",
            "      8       37.1094  0.0831\n",
            "      9       37.1094  0.0911\n",
            "     10       37.1094  0.0786\n",
            "     11       37.1094  0.0860\n",
            "     12       37.1094  0.0822\n",
            "     13       37.1094  0.0808\n",
            "     14       37.1094  0.0908\n",
            "     15       37.1094  0.0820\n",
            "     16       37.1094  0.0767\n",
            "     17       37.1094  0.0734\n",
            "     18       37.1094  0.0758\n",
            "     19       37.1094  0.0823\n",
            "     20       37.1094  0.0813\n",
            "     21       37.1094  0.0645\n",
            "     22       37.1094  0.0721\n",
            "     23       37.1094  0.0670\n",
            "     24       \u001b[36m13.5838\u001b[0m  0.0741\n",
            "     25        \u001b[36m0.6931\u001b[0m  0.0778\n",
            "     26        0.6931  0.0687\n",
            "     27        0.6931  0.0708\n",
            "     28        0.6931  0.0723\n",
            "     29        0.6931  0.0638\n",
            "     30        0.6931  0.0647\n",
            "     31        0.6931  0.0751\n",
            "     32        0.6931  0.0670\n",
            "     33        0.6931  0.0793\n",
            "     34        0.6931  0.0686\n",
            "     35        0.6931  0.0627\n",
            "     36        0.6931  0.0643\n",
            "     37        0.6931  0.0732\n",
            "     38        0.6931  0.0718\n",
            "     39        0.6931  0.0718\n",
            "     40        0.6931  0.0670\n",
            "     41        0.6931  0.0657\n",
            "     42        0.6931  0.0710\n",
            "     43        0.6931  0.0677\n",
            "     44        0.6931  0.0615\n",
            "     45        0.6931  0.0694\n",
            "     46        0.6931  0.0852\n",
            "     47        0.6931  0.0746\n",
            "     48        0.6931  0.0748\n",
            "     49        0.6931  0.0662\n",
            "     50        0.6931  0.0718\n",
            "     51        0.6931  0.0832\n",
            "     52        0.6931  0.0899\n",
            "     53        0.6931  0.0804\n",
            "     54        0.6931  0.0800\n",
            "     55        0.6931  0.0702\n",
            "     56        0.6931  0.0743\n",
            "     57        0.6931  0.0654\n",
            "     58        0.6931  0.0710\n",
            "     59        0.6931  0.0681\n",
            "     60        0.6931  0.0715\n",
            "     61        0.6931  0.0909\n",
            "     62        0.6931  0.0837\n",
            "     63        0.6931  0.0762\n",
            "     64        0.6931  0.0637\n",
            "     65        0.6931  0.0678\n",
            "     66        0.6931  0.0738\n",
            "     67        0.6931  0.0689\n",
            "     68        0.6931  0.0735\n",
            "     69        0.6931  0.0693\n",
            "     70        0.6931  0.0717\n",
            "     71        0.6931  0.0741\n",
            "     72        0.6931  0.0684\n",
            "     73        0.6931  0.0723\n",
            "     74        0.6931  0.0865\n",
            "     75        0.6931  0.0652\n",
            "     76        0.6931  0.0691\n",
            "     77        0.6931  0.0707\n",
            "     78        0.6931  0.0786\n",
            "     79        0.6931  0.0703\n",
            "     80        0.6931  0.0682\n",
            "     81        0.6931  0.0713\n",
            "     82        0.6931  0.0681\n",
            "     83        0.6931  0.0661\n",
            "     84        0.6931  0.0615\n",
            "     85        0.6931  0.0713\n",
            "     86        0.6931  0.0889\n",
            "     87        0.6931  0.0700\n",
            "     88        0.6931  0.0741\n",
            "     89        0.6931  0.0816\n",
            "     90        0.6931  0.0789\n",
            "     91        0.6931  0.0734\n",
            "     92        0.6931  0.0822\n",
            "     93        0.6931  0.0718\n",
            "     94        0.6931  0.0689\n",
            "     95        0.6931  0.0755\n",
            "     96        0.6931  0.0666\n",
            "     97        0.6931  0.0740\n",
            "     98        0.6931  0.0671\n",
            "     99        0.6931  0.0677\n",
            "    100        0.6931  0.0786\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0846\n",
            "      2       37.3047  0.0695\n",
            "      3       37.3047  0.0653\n",
            "      4       37.3047  0.0625\n",
            "      5       37.3047  0.0628\n",
            "      6       37.3047  0.0652\n",
            "      7       37.3047  0.0726\n",
            "      8       37.3047  0.0615\n",
            "      9       37.3047  0.0658\n",
            "     10       37.3047  0.0657\n",
            "     11       37.3047  0.0819\n",
            "     12       37.3047  0.0647\n",
            "     13       37.3047  0.0710\n",
            "     14       37.3047  0.0837\n",
            "     15       37.3047  0.0678\n",
            "     16       37.3047  0.0627\n",
            "     17       37.3047  0.0715\n",
            "     18       37.3047  0.0653\n",
            "     19       37.3047  0.0679\n",
            "     20       37.3047  0.0662\n",
            "     21       37.3047  0.0699\n",
            "     22       37.3047  0.0613\n",
            "     23       37.3047  0.1245\n",
            "     24       37.3047  0.1231\n",
            "     25       37.3047  0.0984\n",
            "     26       \u001b[36m12.9815\u001b[0m  0.1109\n",
            "     27        \u001b[36m0.6931\u001b[0m  0.1018\n",
            "     28        0.6931  0.1209\n",
            "     29        0.6931  0.1015\n",
            "     30        0.6931  0.1045\n",
            "     31        0.6931  0.0898\n",
            "     32        0.6931  0.1005\n",
            "     33        0.6931  0.0924\n",
            "     34        0.6931  0.0973\n",
            "     35        0.6931  0.0972\n",
            "     36        0.6931  0.1165\n",
            "     37        0.6931  0.1123\n",
            "     38        0.6931  0.0991\n",
            "     39        0.6931  0.0943\n",
            "     40        0.6931  0.0968\n",
            "     41        0.6931  0.0876\n",
            "     42        0.6931  0.1104\n",
            "     43        0.6931  0.1030\n",
            "     44        0.6931  0.0956\n",
            "     45        0.6931  0.0931\n",
            "     46        0.6931  0.1100\n",
            "     47        0.6931  0.1081\n",
            "     48        0.6931  0.1015\n",
            "     49        0.6931  0.1025\n",
            "     50        0.6931  0.0969\n",
            "     51        0.6931  0.1070\n",
            "     52        0.6931  0.1072\n",
            "     53        0.6931  0.1026\n",
            "     54        0.6931  0.1058\n",
            "     55        0.6931  0.0841\n",
            "     56        0.6931  0.0746\n",
            "     57        0.6931  0.0941\n",
            "     58        0.6931  0.0660\n",
            "     59        0.6931  0.0695\n",
            "     60        0.6931  0.0648\n",
            "     61        0.6931  0.0733\n",
            "     62        0.6931  0.0851\n",
            "     63        0.6931  0.0710\n",
            "     64        0.6931  0.0642\n",
            "     65        0.6931  0.0687\n",
            "     66        0.6931  0.0646\n",
            "     67        0.6931  0.0682\n",
            "     68        0.6931  0.0693\n",
            "     69        0.6931  0.0634\n",
            "     70        0.6931  0.0780\n",
            "     71        0.6931  0.0815\n",
            "     72        0.6931  0.0777\n",
            "     73        0.6931  0.0667\n",
            "     74        0.6931  0.0721\n",
            "     75        0.6931  0.0629\n",
            "     76        0.6931  0.0711\n",
            "     77        0.6931  0.1053\n",
            "     78        0.6931  0.0857\n",
            "     79        0.6931  0.1132\n",
            "     80        0.6931  0.1048\n",
            "     81        0.6931  0.0820\n",
            "     82        0.6931  0.1165\n",
            "     83        0.6931  0.0917\n",
            "     84        0.6931  0.0740\n",
            "     85        0.6931  0.0695\n",
            "     86        0.6931  0.0657\n",
            "     87        0.6931  0.0644\n",
            "     88        0.6931  0.0656\n",
            "     89        0.6931  0.0638\n",
            "     90        0.6931  0.0785\n",
            "     91        0.6931  0.0846\n",
            "     92        0.6931  0.0862\n",
            "     93        0.6931  0.0891\n",
            "     94        0.6931  0.0896\n",
            "     95        0.6931  0.0968\n",
            "     96        0.6931  0.0810\n",
            "     97        0.6931  0.1082\n",
            "     98        0.6931  0.0894\n",
            "     99        0.6931  0.0843\n",
            "    100        0.6931  0.0820\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0853\n",
            "      2       37.3047  0.0948\n",
            "      3       37.3047  0.0720\n",
            "      4       37.3047  0.0667\n",
            "      5       37.3047  0.0959\n",
            "      6       37.3047  0.1068\n",
            "      7       37.3047  0.0672\n",
            "      8       37.3047  0.0695\n",
            "      9       37.3047  0.0797\n",
            "     10       37.3047  0.0881\n",
            "     11       37.3047  0.0749\n",
            "     12       37.3047  0.0626\n",
            "     13       37.3047  0.0691\n",
            "     14       37.3047  0.0625\n",
            "     15       37.3047  0.0793\n",
            "     16       37.3047  0.0637\n",
            "     17       37.3047  0.0640\n",
            "     18       37.3047  0.0712\n",
            "     19       37.3047  0.0684\n",
            "     20       37.3047  0.0836\n",
            "     21       37.3047  0.0686\n",
            "     22       37.3047  0.0700\n",
            "     23       37.3047  0.0639\n",
            "     24       37.3047  0.0656\n",
            "     25        \u001b[36m8.4825\u001b[0m  0.0725\n",
            "     26        \u001b[36m0.6201\u001b[0m  0.0770\n",
            "     27        \u001b[36m0.6201\u001b[0m  0.0727\n",
            "     28        \u001b[36m0.6201\u001b[0m  0.1019\n",
            "     29        \u001b[36m0.6200\u001b[0m  0.0645\n",
            "     30        \u001b[36m0.6200\u001b[0m  0.0638\n",
            "     31        \u001b[36m0.6200\u001b[0m  0.0709\n",
            "     32        \u001b[36m0.6200\u001b[0m  0.0804\n",
            "     33        \u001b[36m0.6200\u001b[0m  0.0747\n",
            "     34        \u001b[36m0.6200\u001b[0m  0.0812\n",
            "     35        \u001b[36m0.6200\u001b[0m  0.0849\n",
            "     36        \u001b[36m0.6200\u001b[0m  0.0901\n",
            "     37        \u001b[36m0.6200\u001b[0m  0.0802\n",
            "     38        \u001b[36m0.6200\u001b[0m  0.1079\n",
            "     39        \u001b[36m0.6200\u001b[0m  0.0805\n",
            "     40        \u001b[36m0.6200\u001b[0m  0.0900\n",
            "     41        \u001b[36m0.6200\u001b[0m  0.0935\n",
            "     42        \u001b[36m0.6200\u001b[0m  0.0735\n",
            "     43        \u001b[36m0.6200\u001b[0m  0.0894\n",
            "     44        \u001b[36m0.6200\u001b[0m  0.0703\n",
            "     45        \u001b[36m0.6200\u001b[0m  0.0912\n",
            "     46        \u001b[36m0.6200\u001b[0m  0.0725\n",
            "     47        \u001b[36m0.6200\u001b[0m  0.0804\n",
            "     48        \u001b[36m0.6200\u001b[0m  0.0704\n",
            "     49        \u001b[36m0.6200\u001b[0m  0.0820\n",
            "     50        \u001b[36m0.6200\u001b[0m  0.0757\n",
            "     51        \u001b[36m0.6200\u001b[0m  0.0672\n",
            "     52        \u001b[36m0.6200\u001b[0m  0.0695\n",
            "     53        \u001b[36m0.6200\u001b[0m  0.0690\n",
            "     54        \u001b[36m0.6200\u001b[0m  0.0631\n",
            "     55        \u001b[36m0.6200\u001b[0m  0.0649\n",
            "     56        \u001b[36m0.6200\u001b[0m  0.0685\n",
            "     57        \u001b[36m0.6200\u001b[0m  0.0671\n",
            "     58        \u001b[36m0.6200\u001b[0m  0.0811\n",
            "     59        \u001b[36m0.6200\u001b[0m  0.0767\n",
            "     60        \u001b[36m0.6200\u001b[0m  0.0640\n",
            "     61        \u001b[36m0.6200\u001b[0m  0.0647\n",
            "     62        \u001b[36m0.6200\u001b[0m  0.0657\n",
            "     63        \u001b[36m0.6200\u001b[0m  0.0746\n",
            "     64        \u001b[36m0.6200\u001b[0m  0.0686\n",
            "     65        \u001b[36m0.6200\u001b[0m  0.0760\n",
            "     66        \u001b[36m0.6200\u001b[0m  0.0704\n",
            "     67        \u001b[36m0.6200\u001b[0m  0.0645\n",
            "     68        \u001b[36m0.6200\u001b[0m  0.0649\n",
            "     69        \u001b[36m0.6200\u001b[0m  0.0703\n",
            "     70        \u001b[36m0.6200\u001b[0m  0.0626\n",
            "     71        \u001b[36m0.6200\u001b[0m  0.0717\n",
            "     72        \u001b[36m0.6200\u001b[0m  0.0696\n",
            "     73        \u001b[36m0.6200\u001b[0m  0.0646\n",
            "     74        \u001b[36m0.6200\u001b[0m  0.0780\n",
            "     75        \u001b[36m0.6200\u001b[0m  0.0709\n",
            "     76        \u001b[36m0.6200\u001b[0m  0.0719\n",
            "     77        \u001b[36m0.6200\u001b[0m  0.0772\n",
            "     78        \u001b[36m0.6200\u001b[0m  0.0682\n",
            "     79        \u001b[36m0.6200\u001b[0m  0.0818\n",
            "     80        \u001b[36m0.6200\u001b[0m  0.0896\n",
            "     81        \u001b[36m0.6200\u001b[0m  0.1226\n",
            "     82        \u001b[36m0.6200\u001b[0m  0.1200\n",
            "     83        \u001b[36m0.6200\u001b[0m  0.1221\n",
            "     84        \u001b[36m0.6200\u001b[0m  0.1346\n",
            "     85        \u001b[36m0.6200\u001b[0m  0.1244\n",
            "     86        \u001b[36m0.6200\u001b[0m  0.1022\n",
            "     87        \u001b[36m0.6200\u001b[0m  0.1170\n",
            "     88        \u001b[36m0.6200\u001b[0m  0.1488\n",
            "     89        \u001b[36m0.6200\u001b[0m  0.1044\n",
            "     90        \u001b[36m0.6200\u001b[0m  0.1285\n",
            "     91        \u001b[36m0.6200\u001b[0m  0.1276\n",
            "     92        \u001b[36m0.6200\u001b[0m  0.0983\n",
            "     93        \u001b[36m0.6200\u001b[0m  0.1267\n",
            "     94        \u001b[36m0.6200\u001b[0m  0.0907\n",
            "     95        \u001b[36m0.6200\u001b[0m  0.1009\n",
            "     96        \u001b[36m0.6200\u001b[0m  0.1034\n",
            "     97        \u001b[36m0.6200\u001b[0m  0.0963\n",
            "     98        \u001b[36m0.6200\u001b[0m  0.0861\n",
            "     99        \u001b[36m0.6200\u001b[0m  0.0868\n",
            "    100        \u001b[36m0.6200\u001b[0m  0.0916\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0872\n",
            "      2       37.3047  0.1032\n",
            "      3       37.3047  0.1105\n",
            "      4       37.3047  0.0974\n",
            "      5       37.3047  0.0912\n",
            "      6       37.3047  0.1049\n",
            "      7       37.3047  0.0954\n",
            "      8       37.3047  0.1143\n",
            "      9       37.3047  0.0962\n",
            "     10       37.3047  0.0969\n",
            "     11       37.3047  0.0660\n",
            "     12       37.3047  0.0607\n",
            "     13       37.3047  0.0685\n",
            "     14       37.3047  0.0756\n",
            "     15       37.3047  0.0669\n",
            "     16       37.3047  0.0644\n",
            "     17       37.3047  0.0690\n",
            "     18       37.3047  0.0798\n",
            "     19       37.3047  0.0776\n",
            "     20       37.3047  0.0719\n",
            "     21       37.3047  0.0701\n",
            "     22       37.3047  0.0636\n",
            "     23       37.3047  0.0694\n",
            "     24       37.3047  0.0716\n",
            "     25       \u001b[36m15.3124\u001b[0m  0.0659\n",
            "     26        \u001b[36m0.6931\u001b[0m  0.0682\n",
            "     27        0.6931  0.0652\n",
            "     28        0.6931  0.0697\n",
            "     29        0.6931  0.0781\n",
            "     30        0.6931  0.0624\n",
            "     31        0.6931  0.0699\n",
            "     32        0.6931  0.0813\n",
            "     33        0.6931  0.0687\n",
            "     34        0.6931  0.0771\n",
            "     35        0.6931  0.0754\n",
            "     36        0.6931  0.0777\n",
            "     37        0.6931  0.0762\n",
            "     38        0.6931  0.0693\n",
            "     39        0.6931  0.0769\n",
            "     40        0.6931  0.0740\n",
            "     41        0.6931  0.0683\n",
            "     42        0.6931  0.0704\n",
            "     43        0.6931  0.0644\n",
            "     44        0.6931  0.0687\n",
            "     45        0.6931  0.0632\n",
            "     46        0.6931  0.0802\n",
            "     47        0.6931  0.0789\n",
            "     48        0.6931  0.0759\n",
            "     49        0.6931  0.0756\n",
            "     50        0.6931  0.0693\n",
            "     51        0.6931  0.0658\n",
            "     52        0.6931  0.0668\n",
            "     53        0.6931  0.0719\n",
            "     54        0.6931  0.0644\n",
            "     55        0.6931  0.0744\n",
            "     56        0.6931  0.0666\n",
            "     57        0.6931  0.0755\n",
            "     58        0.6931  0.0671\n",
            "     59        0.6931  0.0712\n",
            "     60        0.6931  0.0797\n",
            "     61        0.6931  0.0685\n",
            "     62        0.6931  0.0692\n",
            "     63        0.6931  0.0679\n",
            "     64        0.6931  0.0672\n",
            "     65        0.6931  0.0693\n",
            "     66        0.6931  0.0639\n",
            "     67        0.6931  0.0634\n",
            "     68        0.6931  0.0663\n",
            "     69        0.6931  0.0748\n",
            "     70        0.6931  0.0663\n",
            "     71        0.6931  0.0772\n",
            "     72        0.6931  0.0749\n",
            "     73        0.6931  0.0775\n",
            "     74        0.6931  0.0694\n",
            "     75        0.6931  0.0648\n",
            "     76        0.6931  0.0712\n",
            "     77        0.6931  0.0635\n",
            "     78        0.6931  0.0915\n",
            "     79        0.6931  0.0838\n",
            "     80        0.6931  0.0661\n",
            "     81        0.6931  0.0610\n",
            "     82        0.6931  0.0684\n",
            "     83        0.6931  0.0708\n",
            "     84        0.6931  0.1041\n",
            "     85        0.6931  0.0980\n",
            "     86        0.6931  0.0760\n",
            "     87        0.6931  0.0643\n",
            "     88        0.6931  0.0942\n",
            "     89        0.6931  0.0667\n",
            "     90        0.6931  0.0676\n",
            "     91        0.6931  0.0816\n",
            "     92        0.6931  0.0629\n",
            "     93        0.6931  0.0760\n",
            "     94        0.6931  0.0705\n",
            "     95        0.6931  0.0756\n",
            "     96        0.6931  0.0663\n",
            "     97        0.6931  0.0789\n",
            "     98        0.6931  0.0706\n",
            "     99        0.6931  0.0683\n",
            "    100        0.6931  0.0839\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0729\n",
            "      2       37.3047  0.0769\n",
            "      3       37.3047  0.0762\n",
            "      4       37.3047  0.0692\n",
            "      5       37.3047  0.0640\n",
            "      6       37.3047  0.0642\n",
            "      7       37.3047  0.0667\n",
            "      8       37.3047  0.0735\n",
            "      9       37.3047  0.0632\n",
            "     10       37.3047  0.0669\n",
            "     11       37.3047  0.0721\n",
            "     12       37.3047  0.0777\n",
            "     13       37.3047  0.0616\n",
            "     14       37.3047  0.0638\n",
            "     15       37.3047  0.0655\n",
            "     16       37.3047  0.0654\n",
            "     17       37.3047  0.0733\n",
            "     18       37.3047  0.0665\n",
            "     19       37.3047  0.0625\n",
            "     20       37.3047  0.0841\n",
            "     21       37.3047  0.1899\n",
            "     22       37.3047  0.0887\n",
            "     23       37.3047  0.1018\n",
            "     24       37.3047  0.0818\n",
            "     25       \u001b[36m15.8687\u001b[0m  0.0656\n",
            "     26        \u001b[36m0.6931\u001b[0m  0.0671\n",
            "     27        0.6931  0.0674\n",
            "     28        0.6931  0.0619\n",
            "     29        0.6931  0.0667\n",
            "     30        0.6931  0.0650\n",
            "     31        0.6931  0.0658\n",
            "     32        0.6931  0.0629\n",
            "     33        0.6931  0.0610\n",
            "     34        0.6931  0.0668\n",
            "     35        0.6931  0.0598\n",
            "     36        0.6931  0.0637\n",
            "     37        0.6931  0.0632\n",
            "     38        0.6931  0.0769\n",
            "     39        0.6931  0.0678\n",
            "     40        0.6931  0.0695\n",
            "     41        0.6931  0.0662\n",
            "     42        0.6931  0.0632\n",
            "     43        0.6931  0.0678\n",
            "     44        0.6931  0.0958\n",
            "     45        0.6931  0.0980\n",
            "     46        0.6931  0.0867\n",
            "     47        0.6931  0.1004\n",
            "     48        0.6931  0.0846\n",
            "     49        0.6931  0.0966\n",
            "     50        0.6931  0.1105\n",
            "     51        0.6931  0.0853\n",
            "     52        0.6931  0.0986\n",
            "     53        0.6931  0.0953\n",
            "     54        0.6931  0.1260\n",
            "     55        0.6931  0.1468\n",
            "     56        0.6931  0.0877\n",
            "     57        0.6931  0.0873\n",
            "     58        0.6931  0.0937\n",
            "     59        0.6931  0.0898\n",
            "     60        0.6931  0.0961\n",
            "     61        0.6931  0.0995\n",
            "     62        0.6931  0.0842\n",
            "     63        0.6931  0.0894\n",
            "     64        0.6931  0.0923\n",
            "     65        0.6931  0.0946\n",
            "     66        0.6931  0.1072\n",
            "     67        0.6931  0.0901\n",
            "     68        0.6931  0.0910\n",
            "     69        0.6931  0.1099\n",
            "     70        0.6931  0.1060\n",
            "     71        0.6931  0.0973\n",
            "     72        0.6931  0.1347\n",
            "     73        0.6931  0.1040\n",
            "     74        0.6931  0.1198\n",
            "     75        0.6931  0.1166\n",
            "     76        0.6931  0.0868\n",
            "     77        0.6931  0.0810\n",
            "     78        0.6931  0.0752\n",
            "     79        0.6931  0.0734\n",
            "     80        0.6931  0.0702\n",
            "     81        0.6931  0.0943\n",
            "     82        0.6931  0.0670\n",
            "     83        0.6931  0.0827\n",
            "     84        0.6931  0.0666\n",
            "     85        0.6931  0.0643\n",
            "     86        0.6931  0.0630\n",
            "     87        0.6931  0.0714\n",
            "     88        0.6931  0.0745\n",
            "     89        0.6931  0.0701\n",
            "     90        0.6931  0.0741\n",
            "     91        0.6931  0.0894\n",
            "     92        0.6931  0.0756\n",
            "     93        0.6931  0.0685\n",
            "     94        0.6931  0.0775\n",
            "     95        0.6931  0.0679\n",
            "     96        0.6931  0.0685\n",
            "     97        0.6931  0.0656\n",
            "     98        0.6931  0.0660\n",
            "     99        0.6931  0.0695\n",
            "    100        0.6931  0.0703\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0663\n",
            "      2       37.3047  0.0756\n",
            "      3       37.3047  0.0677\n",
            "      4       37.3047  0.0694\n",
            "      5       37.3047  0.0773\n",
            "      6       37.3047  0.0699\n",
            "      7       37.3047  0.0630\n",
            "      8       37.3047  0.0807\n",
            "      9       37.3047  0.0615\n",
            "     10       37.3047  0.0668\n",
            "     11       37.3047  0.0741\n",
            "     12       37.3047  0.0630\n",
            "     13       37.3047  0.0744\n",
            "     14       37.3047  0.0643\n",
            "     15       37.3047  0.0703\n",
            "     16       37.3047  0.0673\n",
            "     17       37.3047  0.0729\n",
            "     18       37.3047  0.0712\n",
            "     19       37.3047  0.0680\n",
            "     20       37.3047  0.0658\n",
            "     21       37.3047  0.0647\n",
            "     22       37.3047  0.0725\n",
            "     23       37.3047  0.0742\n",
            "     24       37.3047  0.0709\n",
            "     25       \u001b[36m15.8747\u001b[0m  0.0766\n",
            "     26        \u001b[36m0.6931\u001b[0m  0.0692\n",
            "     27        0.6931  0.0765\n",
            "     28        0.6931  0.0873\n",
            "     29        0.6931  0.0745\n",
            "     30        0.6931  0.0729\n",
            "     31        0.6931  0.0797\n",
            "     32        0.6931  0.0714\n",
            "     33        0.6931  0.0664\n",
            "     34        0.6931  0.0872\n",
            "     35        0.6931  0.0842\n",
            "     36        0.6931  0.0801\n",
            "     37        0.6931  0.0658\n",
            "     38        0.6931  0.0630\n",
            "     39        0.6931  0.0660\n",
            "     40        0.6931  0.0679\n",
            "     41        0.6931  0.0652\n",
            "     42        0.6931  0.0682\n",
            "     43        0.6931  0.0678\n",
            "     44        0.6931  0.0649\n",
            "     45        0.6931  0.0672\n",
            "     46        0.6931  0.0633\n",
            "     47        0.6931  0.0718\n",
            "     48        0.6931  0.0685\n",
            "     49        0.6931  0.0656\n",
            "     50        0.6931  0.0758\n",
            "     51        0.6931  0.0717\n",
            "     52        0.6931  0.0687\n",
            "     53        0.6931  0.0610\n",
            "     54        0.6931  0.0652\n",
            "     55        0.6931  0.0644\n",
            "     56        0.6931  0.0614\n",
            "     57        0.6931  0.0670\n",
            "     58        0.6931  0.0637\n",
            "     59        0.6931  0.0732\n",
            "     60        0.6931  0.0784\n",
            "     61        0.6931  0.0668\n",
            "     62        0.6931  0.0654\n",
            "     63        0.6931  0.0685\n",
            "     64        0.6931  0.0681\n",
            "     65        0.6931  0.0874\n",
            "     66        0.6931  0.0674\n",
            "     67        0.6931  0.0676\n",
            "     68        0.6931  0.0684\n",
            "     69        0.6931  0.0691\n",
            "     70        0.6931  0.0653\n",
            "     71        0.6931  0.0692\n",
            "     72        0.6931  0.0711\n",
            "     73        0.6931  0.0747\n",
            "     74        0.6931  0.0728\n",
            "     75        0.6931  0.0726\n",
            "     76        0.6931  0.0666\n",
            "     77        0.6931  0.0719\n",
            "     78        0.6931  0.0732\n",
            "     79        0.6931  0.0875\n",
            "     80        0.6931  0.0676\n",
            "     81        0.6931  0.0688\n",
            "     82        0.6931  0.0665\n",
            "     83        0.6931  0.0779\n",
            "     84        0.6931  0.0731\n",
            "     85        0.6931  0.0646\n",
            "     86        0.6931  0.0725\n",
            "     87        0.6931  0.0688\n",
            "     88        0.6931  0.0699\n",
            "     89        0.6931  0.0668\n",
            "     90        0.6931  0.0696\n",
            "     91        0.6931  0.0655\n",
            "     92        0.6931  0.0700\n",
            "     93        0.6931  0.0931\n",
            "     94        0.6931  0.0726\n",
            "     95        0.6931  0.0863\n",
            "     96        0.6931  0.0898\n",
            "     97        0.6931  0.0955\n",
            "     98        0.6931  0.0858\n",
            "     99        0.6931  0.0836\n",
            "    100        0.6931  0.0816\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0674\n",
            "      2       37.3047  0.0689\n",
            "      3       37.3047  0.0665\n",
            "      4       37.3047  0.0895\n",
            "      5       37.3047  0.0992\n",
            "      6       37.3047  0.0791\n",
            "      7       37.3047  0.0892\n",
            "      8       37.3047  0.0706\n",
            "      9       37.3047  0.1053\n",
            "     10       37.3047  0.1476\n",
            "     11       37.3047  0.1056\n",
            "     12       37.3047  0.1220\n",
            "     13       37.3047  0.1052\n",
            "     14       37.3047  0.1057\n",
            "     15       37.3047  0.1047\n",
            "     16       37.3047  0.1118\n",
            "     17       37.3047  0.1113\n",
            "     18       37.3047  0.1079\n",
            "     19       37.3047  0.1179\n",
            "     20       37.3047  0.1043\n",
            "     21       37.3047  0.0922\n",
            "     22       37.3047  0.0957\n",
            "     23       37.3047  0.0885\n",
            "     24       37.3047  0.0850\n",
            "     25       37.3047  0.0849\n",
            "     26       37.3047  0.0842\n",
            "     27       37.3047  0.0801\n",
            "     28       37.3047  0.0879\n",
            "     29       37.3047  0.0827\n",
            "     30       37.3047  0.1065\n",
            "     31       37.3047  0.1071\n",
            "     32       37.3047  0.0910\n",
            "     33       37.3047  0.1433\n",
            "     34       37.3047  0.1173\n",
            "     35       37.3047  0.1345\n",
            "     36       37.3047  0.1394\n",
            "     37       37.3047  0.1404\n",
            "     38       37.3047  0.1218\n",
            "     39       37.3047  0.1401\n",
            "     40       37.3047  0.0915\n",
            "     41       37.3047  0.0850\n",
            "     42       37.3047  0.0753\n",
            "     43       37.3047  0.0669\n",
            "     44       37.3047  0.0876\n",
            "     45       37.3047  0.0677\n",
            "     46       37.3047  0.0747\n",
            "     47       37.3047  0.0666\n",
            "     48       37.3047  0.0747\n",
            "     49       37.3047  0.0711\n",
            "     50       37.3047  0.0743\n",
            "     51       37.3047  0.0733\n",
            "     52       37.3047  0.0704\n",
            "     53       37.3047  0.0700\n",
            "     54       37.3047  0.0731\n",
            "     55       37.3047  0.0662\n",
            "     56       37.3047  0.0713\n",
            "     57       37.3047  0.0763\n",
            "     58       37.3047  0.0892\n",
            "     59       37.3047  0.0828\n",
            "     60       37.3047  0.0750\n",
            "     61       37.3047  0.0677\n",
            "     62       37.3047  0.0671\n",
            "     63       37.3047  0.0673\n",
            "     64       37.3047  0.0672\n",
            "     65       37.3047  0.0645\n",
            "     66       37.3047  0.0644\n",
            "     67       37.3047  0.0600\n",
            "     68       37.3047  0.0717\n",
            "     69       37.3047  0.0678\n",
            "     70       37.3047  0.0613\n",
            "     71       37.3047  0.0659\n",
            "     72       37.3047  0.0803\n",
            "     73       37.3047  0.0679\n",
            "     74       37.3047  0.0690\n",
            "     75       37.3047  0.0761\n",
            "     76       37.3047  0.0719\n",
            "     77       37.3047  0.0638\n",
            "     78       37.3047  0.0714\n",
            "     79       37.3047  0.0711\n",
            "     80       37.3047  0.0733\n",
            "     81       37.3047  0.0671\n",
            "     82       37.3047  0.0678\n",
            "     83       37.3047  0.0630\n",
            "     84       37.3047  0.0649\n",
            "     85       37.3047  0.0762\n",
            "     86       37.3047  0.0720\n",
            "     87       37.3047  0.0677\n",
            "     88       37.3047  0.0671\n",
            "     89       37.3047  0.0771\n",
            "     90       37.3047  0.0700\n",
            "     91       37.3047  0.0689\n",
            "     92       37.3047  0.0798\n",
            "     93       37.3047  0.0649\n",
            "     94       37.3047  0.0697\n",
            "     95       37.3047  0.0704\n",
            "     96       37.3047  0.0664\n",
            "     97       37.3047  0.0763\n",
            "     98       37.3047  0.0680\n",
            "     99       37.3047  0.0684\n",
            "    100       37.3047  0.0623\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.3047\u001b[0m  0.0711\n",
            "      2       37.3047  0.0698\n",
            "      3       37.3047  0.0861\n",
            "      4       37.3047  0.0790\n",
            "      5       37.3047  0.0701\n",
            "      6       37.3047  0.0834\n",
            "      7       37.3047  0.0692\n",
            "      8       37.3047  0.0629\n",
            "      9       37.3047  0.0702\n",
            "     10       37.3047  0.0622\n",
            "     11       37.3047  0.0626\n",
            "     12       37.3047  0.0692\n",
            "     13       37.3047  0.0753\n",
            "     14       37.3047  0.0701\n",
            "     15       37.3047  0.0696\n",
            "     16       37.3047  0.0708\n",
            "     17       37.3047  0.0660\n",
            "     18       37.3047  0.0630\n",
            "     19       37.3047  0.0661\n",
            "     20       37.3047  0.0734\n",
            "     21       37.3047  0.0682\n",
            "     22       37.3047  0.0663\n",
            "     23       37.3047  0.0709\n",
            "     24       37.3047  0.0666\n",
            "     25       \u001b[36m16.2907\u001b[0m  0.0684\n",
            "     26        \u001b[36m0.6607\u001b[0m  0.0760\n",
            "     27        \u001b[36m0.6607\u001b[0m  0.0629\n",
            "     28        0.6607  0.0611\n",
            "     29        0.6607  0.0867\n",
            "     30        0.6607  0.0678\n",
            "     31        0.6607  0.0664\n",
            "     32        0.6607  0.0621\n",
            "     33        0.6607  0.0739\n",
            "     34        0.6607  0.0671\n",
            "     35        0.6607  0.0643\n",
            "     36        0.6607  0.0651\n",
            "     37        0.6607  0.0686\n",
            "     38        0.6607  0.0687\n",
            "     39        0.6607  0.0633\n",
            "     40        0.6607  0.0718\n",
            "     41        0.6607  0.0680\n",
            "     42        0.6607  0.0660\n",
            "     43        0.6607  0.0715\n",
            "     44        0.6607  0.0910\n",
            "     45        0.6607  0.0777\n",
            "     46        0.6607  0.0691\n",
            "     47        0.6607  0.0697\n",
            "     48        0.6607  0.0634\n",
            "     49        0.6607  0.0663\n",
            "     50        0.6607  0.0663\n",
            "     51        0.6607  0.0750\n",
            "     52        0.6607  0.0745\n",
            "     53        0.6607  0.0712\n",
            "     54        0.6607  0.0736\n",
            "     55        0.6607  0.0749\n",
            "     56        0.6607  0.0787\n",
            "     57        0.6607  0.0915\n",
            "     58        0.6607  0.0724\n",
            "     59        0.6607  0.0673\n",
            "     60        0.6607  0.0657\n",
            "     61        0.6607  0.0654\n",
            "     62        0.6607  0.0665\n",
            "     63        0.6607  0.0651\n",
            "     64        0.6607  0.0596\n",
            "     65        0.6607  0.0637\n",
            "     66        0.6607  0.0628\n",
            "     67        0.6607  0.0693\n",
            "     68        0.6607  0.0664\n",
            "     69        0.6607  0.0681\n",
            "     70        0.6607  0.0701\n",
            "     71        0.6607  0.0733\n",
            "     72        0.6607  0.0792\n",
            "     73        0.6607  0.0653\n",
            "     74        0.6607  0.0668\n",
            "     75        0.6607  0.0710\n",
            "     76        0.6607  0.0723\n",
            "     77        0.6607  0.1043\n",
            "     78        0.6607  0.0965\n",
            "     79        0.6607  0.0882\n",
            "     80        0.6607  0.0938\n",
            "     81        0.6607  0.0904\n",
            "     82        0.6607  0.0966\n",
            "     83        0.6607  0.1439\n",
            "     84        0.6607  0.0921\n",
            "     85        0.6607  0.0938\n",
            "     86        0.6607  0.0947\n",
            "     87        0.6607  0.0932\n",
            "     88        0.6607  0.0924\n",
            "     89        0.6607  0.0865\n",
            "     90        0.6607  0.1203\n",
            "     91        0.6607  0.1214\n",
            "     92        0.6607  0.1368\n",
            "     93        0.6607  0.1138\n",
            "     94        0.6607  0.1039\n",
            "     95        0.6607  0.0927\n",
            "     96        0.6607  0.0875\n",
            "     97        0.6607  0.1082\n",
            "     98        0.6607  0.0873\n",
            "     99        0.6607  0.2462\n",
            "    100        0.6607  0.2317\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m37.2320\u001b[0m  0.1207\n",
            "      2       37.2320  0.2982\n",
            "      3       37.2320  0.1170\n",
            "      4       37.2320  0.1284\n",
            "      5       37.2320  0.0710\n",
            "      6       37.2320  0.0657\n",
            "      7       37.2320  0.0647\n",
            "      8       37.2320  0.0777\n",
            "      9       37.2320  0.0776\n",
            "     10       37.2320  0.0769\n",
            "     11       37.2320  0.0753\n",
            "     12       37.2320  0.0701\n",
            "     13       37.2320  0.0668\n",
            "     14       37.2320  0.0625\n",
            "     15       37.2320  0.0696\n",
            "     16       37.2320  0.0666\n",
            "     17       37.2320  0.0676\n",
            "     18       37.2320  0.0618\n",
            "     19       37.2320  0.0680\n",
            "     20       37.2320  0.0696\n",
            "     21       37.2320  0.0662\n",
            "     22       37.2320  0.0717\n",
            "     23       37.2320  0.0782\n",
            "     24       37.2320  0.0677\n",
            "     25       37.2320  0.0662\n",
            "     26       37.2320  0.0645\n",
            "     27       37.2320  0.0628\n",
            "     28       37.2320  0.0645\n",
            "     29       37.2320  0.0734\n",
            "     30       37.2320  0.0684\n",
            "     31       37.2320  0.0633\n",
            "     32       37.2320  0.0646\n",
            "     33       37.2320  0.0757\n",
            "     34       37.2320  0.0676\n",
            "     35       37.2320  0.0606\n",
            "     36       37.2320  0.0623\n",
            "     37       37.2320  0.0651\n",
            "     38       37.2320  0.0836\n",
            "     39       37.2320  0.0633\n",
            "     40       37.2320  0.0658\n",
            "     41       37.2320  0.0680\n",
            "     42       37.2320  0.0670\n",
            "     43       37.2320  0.0632\n",
            "     44       37.2320  0.0633\n",
            "     45       37.2320  0.0649\n",
            "     46       37.2320  0.0669\n",
            "     47       37.2320  0.0788\n",
            "     48       37.2320  0.0651\n",
            "     49       37.2320  0.0610\n",
            "     50       37.2320  0.0789\n",
            "     51       37.2320  0.0741\n",
            "     52       37.2320  0.0757\n",
            "     53       37.2320  0.0724\n",
            "     54       37.2320  0.0628\n",
            "     55       37.2320  0.0898\n",
            "     56       37.2320  0.1157\n",
            "     57       37.2320  0.1067\n",
            "     58       37.2320  0.1139\n",
            "     59       37.2320  0.1448\n",
            "     60       37.2320  0.1277\n",
            "     61       37.2320  0.1693\n",
            "     62       37.2320  0.0783\n",
            "     63       37.2320  0.0664\n",
            "     64       37.2320  0.0666\n",
            "     65       37.2320  0.0981\n",
            "     66       37.2320  0.0642\n",
            "     67       37.2320  0.0683\n",
            "     68       37.2320  0.0734\n",
            "     69       37.2320  0.0746\n",
            "     70       37.2320  0.0834\n",
            "     71       37.2320  0.0680\n",
            "     72       37.2320  0.0924\n",
            "     73       37.2320  0.0760\n",
            "     74       37.2320  0.1023\n",
            "     75       37.2320  0.0775\n",
            "     76       37.2320  0.0779\n",
            "     77       37.2320  0.0683\n",
            "     78       37.2320  0.0697\n",
            "     79       37.2320  0.0740\n",
            "     80       37.2320  0.0908\n",
            "     81       37.2320  0.0693\n",
            "     82       37.2320  0.0764\n",
            "     83       37.2320  0.0677\n",
            "     84       37.2320  0.0674\n",
            "     85       37.2320  0.0664\n",
            "     86       37.2320  0.0995\n",
            "     87       37.2320  0.1546\n",
            "     88       37.2320  0.1155\n",
            "     89       37.2320  0.1180\n",
            "     90       37.2320  0.1128\n",
            "     91       37.2320  0.1195\n",
            "     92       37.2320  0.0788\n",
            "     93       37.2320  0.0799\n",
            "     94       37.2320  0.0743\n",
            "     95       37.2320  0.0671\n",
            "     96       37.2320  0.0684\n",
            "     97       37.2320  0.0757\n",
            "     98       37.2320  0.0649\n",
            "     99       37.2320  0.0636\n",
            "    100       37.2320  0.0712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultados.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jvuz_0qSMpE",
        "outputId": "19496890-318d-429d-a53e-ff8593b93510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultados"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCUg3MokSaS2",
        "outputId": "4bcdb633-bdf3-4288-ee7c-6e4d4507cbee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.61403509, 0.38596491, 0.36842105, 0.43859649, 0.36842105,\n",
              "       0.36842105, 0.36842105, 0.63157895, 0.43859649, 0.625     ])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "media = resultados.mean()\n",
        "media"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU2QBrM4Si--",
        "outputId": "9aa8ffeb-635d-4188-a42b-83f8dfcb50ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4607456140350877"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "desvio_padrao = resultados.std()\n",
        "desvio_padrao"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWysLd19Szdy",
        "outputId": "161bf7c7-8367-4e07-f3d2-1e307ba522dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10969494656518622"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Etapa 6: Dropout"
      ],
      "metadata": {
        "id": "FdO3c3IsSN06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class classificador_torch(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # 30 -> 16 -> 16 -> 1\n",
        "    self.dense0 = nn.Linear(30,16)\n",
        "    torch.nn.init.uniform_(self.dense0.weight)\n",
        "    self.activation0 = nn.ReLU()\n",
        "    self.dropout0 = nn.Dropout(0.2)\n",
        "\n",
        "    self.dense1 = nn.Linear(16,16)\n",
        "    torch.nn.init.uniform_(self.dense1.weight)\n",
        "    self.activation1 = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout(0.2)\n",
        "\n",
        "    self.dense2 = nn.Linear(16,1)\n",
        "    torch.nn.init.uniform_(self.dense2.weight)\n",
        "    self.activation2 = nn.ReLU()\n",
        "    self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "    self.output = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.dense0(x)\n",
        "    x = self.activation0(x)\n",
        "    x = self.dropout0(x)\n",
        "    x = self.dense1(x)\n",
        "    x = self.activation1(x)\n",
        "    x = self.dropout1(x)\n",
        "    x = self.dense2(x)\n",
        "    x = self.activation2(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.output(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "zlqzgrCMt2aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classificador_sklearn = NeuralNetBinaryClassifier(\n",
        "    module=classificador_torch,\n",
        "    criterion=torch.nn.BCELoss,\n",
        "    optimizer =torch.optim.Adam,\n",
        "    lr=0.001,\n",
        "    optimizer__weight_decay=0.0001,\n",
        "    max_epochs=100,\n",
        "    batch_size=10,\n",
        "    train_split=False\n",
        "    )"
      ],
      "metadata": {
        "id": "_fo5Wo0iulxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultados= cross_val_score(classificador_sklearn, previsores, classe, cv=10 , scoring= 'accuracy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD8vJdn-utyS",
        "outputId": "723a9e0f-1e31-4a3e-8d73-60dbc41bd010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m31.1738\u001b[0m  0.1911\n",
            "      2       \u001b[36m30.4061\u001b[0m  0.1853\n",
            "      3       \u001b[36m29.2437\u001b[0m  0.1958\n",
            "      4       31.5563  0.1923\n",
            "      5       30.2176  0.1760\n",
            "      6       29.4228  0.1090\n",
            "      7       30.9826  0.1021\n",
            "      8       \u001b[36m28.8490\u001b[0m  0.1036\n",
            "      9       \u001b[36m28.6456\u001b[0m  0.1131\n",
            "     10       29.6249  0.1407\n",
            "     11       28.6632  0.1012\n",
            "     12       29.0592  0.1144\n",
            "     13       29.2342  0.1011\n",
            "     14       30.4156  0.1291\n",
            "     15       31.1847  0.1037\n",
            "     16       30.9880  0.0938\n",
            "     17       29.2451  0.0816\n",
            "     18       29.8107  0.0963\n",
            "     19       29.0322  0.1503\n",
            "     20       29.6303  0.0998\n",
            "     21       31.3800  0.1204\n",
            "     22       30.4020  0.1034\n",
            "     23       32.3484  0.1087\n",
            "     24       31.5740  0.0851\n",
            "     25       \u001b[36m16.2458\u001b[0m  0.0830\n",
            "     26        \u001b[36m0.6931\u001b[0m  0.0803\n",
            "     27        0.6931  0.0770\n",
            "     28        0.6931  0.0938\n",
            "     29        0.6931  0.1345\n",
            "     30        \u001b[36m0.6918\u001b[0m  0.0875\n",
            "     31        0.6931  0.0775\n",
            "     32        0.6931  0.1067\n",
            "     33        0.6931  0.0896\n",
            "     34        0.6931  0.0804\n",
            "     35        0.6931  0.0871\n",
            "     36        0.6931  0.0771\n",
            "     37        0.6931  0.0830\n",
            "     38        0.6931  0.0816\n",
            "     39        0.6931  0.0791\n",
            "     40        0.6931  0.0838\n",
            "     41        0.6931  0.1024\n",
            "     42        0.6931  0.0906\n",
            "     43        0.6931  0.0793\n",
            "     44        0.6931  0.0836\n",
            "     45        0.6931  0.0982\n",
            "     46        0.6931  0.1140\n",
            "     47        0.6931  0.0833\n",
            "     48        \u001b[36m0.6916\u001b[0m  0.0796\n",
            "     49        0.6931  0.0763\n",
            "     50        0.6931  0.0789\n",
            "     51        0.6931  0.0895\n",
            "     52        0.6931  0.1093\n",
            "     53        0.6931  0.0920\n",
            "     54        0.6931  0.1050\n",
            "     55        0.6931  0.1183\n",
            "     56        0.6931  0.1260\n",
            "     57        0.6931  0.1265\n",
            "     58        0.6931  0.1110\n",
            "     59        0.6931  0.0919\n",
            "     60        0.6931  0.0983\n",
            "     61        0.6931  0.1058\n",
            "     62        0.6931  0.0778\n",
            "     63        0.6931  0.0968\n",
            "     64        0.6931  0.0798\n",
            "     65        0.6931  0.0918\n",
            "     66        0.6931  0.0883\n",
            "     67        0.6931  0.0745\n",
            "     68        0.6931  0.0799\n",
            "     69        0.6931  0.0758\n",
            "     70        0.6931  0.0789\n",
            "     71        0.6931  0.0757\n",
            "     72        0.6931  0.0721\n",
            "     73        0.6931  0.0846\n",
            "     74        0.6931  0.0861\n",
            "     75        0.6931  0.0779\n",
            "     76        0.6931  0.0748\n",
            "     77        0.6931  0.0776\n",
            "     78        0.6931  0.0865\n",
            "     79        0.6931  0.0747\n",
            "     80        0.6931  0.0859\n",
            "     81        0.6931  0.0784\n",
            "     82        0.6931  0.0870\n",
            "     83        0.6931  0.0768\n",
            "     84        0.6931  0.0848\n",
            "     85        0.6931  0.0997\n",
            "     86        0.6931  0.1001\n",
            "     87        0.6931  0.0772\n",
            "     88        0.6931  0.0840\n",
            "     89        0.6931  0.0913\n",
            "     90        0.6931  0.0773\n",
            "     91        0.6931  0.0899\n",
            "     92        0.6932  0.0879\n",
            "     93        0.6931  0.0817\n",
            "     94        0.6931  0.0732\n",
            "     95        0.6931  0.0828\n",
            "     96        0.6931  0.0766\n",
            "     97        0.6931  0.0917\n",
            "     98        0.6931  0.0807\n",
            "     99        0.6931  0.0918\n",
            "    100        0.6931  0.0858\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m30.4142\u001b[0m  0.0748\n",
            "      2       \u001b[36m29.6370\u001b[0m  0.0767\n",
            "      3       30.7981  0.0828\n",
            "      4       \u001b[36m28.8653\u001b[0m  0.0721\n",
            "      5       30.2162  0.0706\n",
            "      6       \u001b[36m27.4927\u001b[0m  0.0733\n",
            "      7       30.4224  0.0811\n",
            "      8       29.6398  0.0758\n",
            "      9       30.0128  0.0829\n",
            "     10       31.9442  0.1006\n",
            "     11       30.9920  0.0784\n",
            "     12       30.6014  0.1069\n",
            "     13       29.8188  0.1203\n",
            "     14       31.1779  0.1246\n",
            "     15       29.8188  0.1067\n",
            "     16       29.2491  0.1171\n",
            "     17       30.5798  0.1177\n",
            "     18       31.3732  0.1526\n",
            "     19       31.3732  0.1621\n",
            "     20       28.8395  0.1512\n",
            "     21       27.6771  0.1488\n",
            "     22       32.1558  0.1340\n",
            "     23       30.3966  0.1257\n",
            "     24       29.6357  0.0996\n",
            "     25       29.4499  0.1219\n",
            "     26       28.6578  0.1790\n",
            "     27       29.8202  0.1292\n",
            "     28       30.2067  0.1084\n",
            "     29       30.6082  0.1183\n",
            "     30       31.1847  0.1188\n",
            "     31       29.0646  0.1248\n",
            "     32       30.6001  0.1496\n",
            "     33       30.5974  0.1247\n",
            "     34       29.6276  0.1446\n",
            "     35       29.8229  0.1331\n",
            "     36       \u001b[36m10.9302\u001b[0m  0.1001\n",
            "     37        \u001b[36m0.6931\u001b[0m  0.0906\n",
            "     38        0.6931  0.0849\n",
            "     39        0.6931  0.0806\n",
            "     40        0.6931  0.0832\n",
            "     41        0.6931  0.0810\n",
            "     42        0.6931  0.0836\n",
            "     43        0.6931  0.0869\n",
            "     44        0.6931  0.0891\n",
            "     45        0.6931  0.1017\n",
            "     46        0.6931  0.0780\n",
            "     47        0.6931  0.0930\n",
            "     48        0.6931  0.0815\n",
            "     49        0.6931  0.0762\n",
            "     50        0.6931  0.0816\n",
            "     51        0.6931  0.0741\n",
            "     52        0.6931  0.0790\n",
            "     53        0.6931  0.0887\n",
            "     54        0.6931  0.1108\n",
            "     55        0.6931  0.0823\n",
            "     56        0.6931  0.0821\n",
            "     57        0.6931  0.0932\n",
            "     58        0.6931  0.0845\n",
            "     59        0.6931  0.0793\n",
            "     60        0.6931  0.0786\n",
            "     61        0.6931  0.0800\n",
            "     62        0.6931  0.0860\n",
            "     63        0.6931  0.0864\n",
            "     64        0.6931  0.0851\n",
            "     65        0.6931  0.0918\n",
            "     66        0.6931  0.0865\n",
            "     67        0.6931  0.0804\n",
            "     68        0.6931  0.0822\n",
            "     69        0.6931  0.1020\n",
            "     70        0.6931  0.0777\n",
            "     71        0.6931  0.0805\n",
            "     72        0.6931  0.0815\n",
            "     73        0.6931  0.0898\n",
            "     74        0.6931  0.0750\n",
            "     75        0.6931  0.0817\n",
            "     76        0.6931  0.0856\n",
            "     77        0.6931  0.0787\n",
            "     78        0.6931  0.0904\n",
            "     79        0.6931  0.0844\n",
            "     80        0.6931  0.0719\n",
            "     81        0.6931  0.0956\n",
            "     82        0.6931  0.0773\n",
            "     83        0.6931  0.0811\n",
            "     84        0.6931  0.0841\n",
            "     85        0.6931  0.0951\n",
            "     86        0.6931  0.0764\n",
            "     87        0.6931  0.0855\n",
            "     88        0.6931  0.0817\n",
            "     89        0.6931  0.0992\n",
            "     90        0.6931  0.0875\n",
            "     91        0.6931  0.0763\n",
            "     92        0.6931  0.0952\n",
            "     93        0.6931  0.0904\n",
            "     94        0.6931  0.0852\n",
            "     95        0.6931  0.0849\n",
            "     96        0.6931  0.0818\n",
            "     97        0.6931  0.0823\n",
            "     98        0.6931  0.0855\n",
            "     99        0.6931  0.0901\n",
            "    100        0.6931  0.0884\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m29.8418\u001b[0m  0.0802\n",
            "      2       30.6190  0.0933\n",
            "      3       \u001b[36m29.4404\u001b[0m  0.0813\n",
            "      4       29.8215  0.1025\n",
            "      5       \u001b[36m27.4927\u001b[0m  0.0730\n",
            "      6       29.8283  0.0706\n",
            "      7       30.4224  0.0969\n",
            "      8       29.4323  0.0849\n",
            "      9       30.7967  0.0764\n",
            "     10       29.8364  0.0947\n",
            "     11       30.7873  0.0975\n",
            "     12       29.6167  0.1107\n",
            "     13       30.9880  0.0999\n",
            "     14       30.9812  0.0793\n",
            "     15       30.5906  0.0977\n",
            "     16       31.3786  0.0766\n",
            "     17       30.6041  0.0865\n",
            "     18       29.8202  0.0811\n",
            "     19       29.6276  0.0723\n",
            "     20       27.8914  0.0798\n",
            "     21       29.0416  0.0714\n",
            "     22       30.9785  0.0787\n",
            "     23       31.7665  0.0843\n",
            "     24       30.2270  0.0838\n",
            "     25       29.2518  0.0758\n",
            "     26       31.7760  0.0803\n",
            "     27       28.8409  0.0833\n",
            "     28       29.4350  0.0825\n",
            "     29       30.2257  0.2412\n",
            "     30        \u001b[36m9.5797\u001b[0m  0.0900\n",
            "     31        \u001b[36m0.6499\u001b[0m  0.0829\n",
            "     32        \u001b[36m0.6420\u001b[0m  0.0814\n",
            "     33        \u001b[36m0.6374\u001b[0m  0.0885\n",
            "     34        0.6420  0.0842\n",
            "     35        0.6495  0.0934\n",
            "     36        \u001b[36m0.6320\u001b[0m  0.1095\n",
            "     37        \u001b[36m0.6147\u001b[0m  0.1048\n",
            "     38        0.6398  0.1012\n",
            "     39        \u001b[36m0.5985\u001b[0m  0.0812\n",
            "     40        0.6102  0.0963\n",
            "     41        0.6190  0.1294\n",
            "     42        0.5987  0.0986\n",
            "     43        \u001b[36m0.5940\u001b[0m  0.0771\n",
            "     44        0.6028  0.0840\n",
            "     45        0.6007  0.0801\n",
            "     46        0.6110  0.0753\n",
            "     47        \u001b[36m0.5937\u001b[0m  0.1219\n",
            "     48        \u001b[36m0.5888\u001b[0m  0.1517\n",
            "     49        \u001b[36m0.5830\u001b[0m  0.1108\n",
            "     50        0.6156  0.1155\n",
            "     51        \u001b[36m0.5824\u001b[0m  0.1109\n",
            "     52        0.5971  0.1275\n",
            "     53        0.5940  0.1166\n",
            "     54        0.5871  0.1090\n",
            "     55        0.6151  0.1026\n",
            "     56        \u001b[36m0.5665\u001b[0m  0.1244\n",
            "     57        0.5985  0.1166\n",
            "     58        0.6035  0.1107\n",
            "     59        0.5916  0.1332\n",
            "     60        0.5901  0.1321\n",
            "     61        0.6087  0.1155\n",
            "     62        0.5930  0.1159\n",
            "     63        0.5889  0.1008\n",
            "     64        0.5965  0.1403\n",
            "     65        0.6236  0.1169\n",
            "     66        0.5844  0.1205\n",
            "     67        0.5873  0.1133\n",
            "     68        0.6121  0.1044\n",
            "     69        0.6069  0.1127\n",
            "     70        0.5919  0.1123\n",
            "     71        0.5763  0.1158\n",
            "     72        0.6014  0.1604\n",
            "     73        0.5685  0.1006\n",
            "     74        0.5851  0.0770\n",
            "     75        0.5927  0.0922\n",
            "     76        0.5941  0.0797\n",
            "     77        0.5827  0.0925\n",
            "     78        0.5985  0.0816\n",
            "     79        0.5969  0.1012\n",
            "     80        \u001b[36m0.5650\u001b[0m  0.0825\n",
            "     81        0.5798  0.0724\n",
            "     82        0.5898  0.0792\n",
            "     83        0.5897  0.0819\n",
            "     84        0.5842  0.0749\n",
            "     85        0.5864  0.0935\n",
            "     86        0.6138  0.0740\n",
            "     87        0.5922  0.0782\n",
            "     88        0.5725  0.0858\n",
            "     89        0.5893  0.0818\n",
            "     90        0.6004  0.0762\n",
            "     91        0.5854  0.0754\n",
            "     92        0.5843  0.0738\n",
            "     93        0.6023  0.0944\n",
            "     94        0.5703  0.0894\n",
            "     95        0.5980  0.0960\n",
            "     96        0.5788  0.0871\n",
            "     97        0.5893  0.0974\n",
            "     98        0.5669  0.0979\n",
            "     99        0.5780  0.0974\n",
            "    100        0.5936  0.0879\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m29.8120\u001b[0m  0.0867\n",
            "      2       \u001b[36m29.0565\u001b[0m  0.0843\n",
            "      3       \u001b[36m27.7042\u001b[0m  0.0967\n",
            "      4       29.8242  0.0805\n",
            "      5       31.7814  0.0917\n",
            "      6       30.8089  0.0802\n",
            "      7       30.4169  0.0825\n",
            "      8       29.0619  0.0877\n",
            "      9       30.6177  0.0795\n",
            "     10       30.2216  0.0794\n",
            "     11       28.8639  0.0731\n",
            "     12       \u001b[36m27.0953\u001b[0m  0.0911\n",
            "     13       30.0344  0.0789\n",
            "     14       27.8725  0.0706\n",
            "     15       29.0565  0.0836\n",
            "     16       30.5974  0.0746\n",
            "     17       30.0047  0.0857\n",
            "     18       30.9975  0.0738\n",
            "     19       29.0633  0.0792\n",
            "     20       30.8103  0.0811\n",
            "     21       29.8351  0.1030\n",
            "     22       \u001b[36m20.2694\u001b[0m  0.0800\n",
            "     23        \u001b[36m0.6931\u001b[0m  0.0799\n",
            "     24        0.6931  0.0829\n",
            "     25        0.6931  0.0751\n",
            "     26        0.6931  0.0707\n",
            "     27        0.6931  0.0808\n",
            "     28        0.6931  0.0850\n",
            "     29        0.6931  0.0783\n",
            "     30        0.6931  0.0889\n",
            "     31        0.6931  0.0805\n",
            "     32        0.6931  0.0800\n",
            "     33        0.6931  0.1001\n",
            "     34        0.6931  0.0905\n",
            "     35        0.6931  0.0825\n",
            "     36        0.6931  0.0904\n",
            "     37        0.6931  0.0803\n",
            "     38        0.6931  0.0802\n",
            "     39        0.6931  0.0712\n",
            "     40        0.6931  0.0738\n",
            "     41        0.6931  0.0800\n",
            "     42        0.6931  0.0866\n",
            "     43        0.6931  0.0801\n",
            "     44        0.6931  0.0904\n",
            "     45        0.6931  0.0920\n",
            "     46        0.6931  0.0805\n",
            "     47        0.6931  0.0812\n",
            "     48        0.6931  0.1051\n",
            "     49        0.6931  0.0822\n",
            "     50        0.6931  0.0914\n",
            "     51        0.6931  0.0851\n",
            "     52        0.6931  0.1003\n",
            "     53        0.6931  0.0798\n",
            "     54        0.6931  0.0919\n",
            "     55        0.6931  0.1641\n",
            "     56        0.6931  0.1901\n",
            "     57        0.6931  0.2127\n",
            "     58        0.6931  0.1445\n",
            "     59        0.6931  0.1322\n",
            "     60        0.6931  0.1382\n",
            "     61        0.6931  0.0976\n",
            "     62        0.6931  0.1299\n",
            "     63        0.6931  0.0880\n",
            "     64        0.6931  0.1177\n",
            "     65        0.6931  0.0874\n",
            "     66        0.6931  0.0945\n",
            "     67        0.6931  0.0805\n",
            "     68        0.6931  0.0876\n",
            "     69        0.6931  0.0853\n",
            "     70        0.6931  0.0953\n",
            "     71        \u001b[36m0.6918\u001b[0m  0.0894\n",
            "     72        0.6931  0.0907\n",
            "     73        0.6931  0.1027\n",
            "     74        0.6931  0.0933\n",
            "     75        0.6931  0.0831\n",
            "     76        0.6931  0.0872\n",
            "     77        0.6931  0.0848\n",
            "     78        0.6931  0.0861\n",
            "     79        0.6931  0.0948\n",
            "     80        0.6931  0.1284\n",
            "     81        0.6931  0.1144\n",
            "     82        0.6931  0.1112\n",
            "     83        0.6931  0.1354\n",
            "     84        0.6931  0.1255\n",
            "     85        0.6931  0.1187\n",
            "     86        0.6931  0.1178\n",
            "     87        0.6931  0.1156\n",
            "     88        0.6931  0.1093\n",
            "     89        0.6931  0.1122\n",
            "     90        0.6931  0.1344\n",
            "     91        0.6931  0.1285\n",
            "     92        0.6931  0.1175\n",
            "     93        0.6931  0.1227\n",
            "     94        0.6931  0.1182\n",
            "     95        0.6931  0.1191\n",
            "     96        0.6931  0.1176\n",
            "     97        0.6931  0.1100\n",
            "     98        0.6931  0.1166\n",
            "     99        0.6931  0.1302\n",
            "    100        0.6931  0.1201\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m27.7042\u001b[0m  0.1123\n",
            "      2       28.0827  0.1151\n",
            "      3       29.4404  0.1124\n",
            "      4       30.4115  0.1234\n",
            "      5       30.4142  0.0948\n",
            "      6       \u001b[36m27.6920\u001b[0m  0.0820\n",
            "      7       30.9934  0.0790\n",
            "      8       30.1986  0.0860\n",
            "      9       30.4075  0.0875\n",
            "     10       32.7472  0.0732\n",
            "     11       31.5699  0.0763\n",
            "     12       30.4102  0.0711\n",
            "     13       28.4584  0.0733\n",
            "     14       29.4404  0.0759\n",
            "     15       28.2780  0.0758\n",
            "     16       30.8157  0.0857\n",
            "     17       30.4156  0.0710\n",
            "     18       30.2216  0.0772\n",
            "     19       29.0416  0.0928\n",
            "     20       29.0403  0.0771\n",
            "     21       30.2203  0.0916\n",
            "     22       29.4363  0.0800\n",
            "     23       28.6754  0.0870\n",
            "     24       30.9920  0.0834\n",
            "     25       31.6227  0.0731\n",
            "     26        \u001b[36m4.2352\u001b[0m  0.0924\n",
            "     27        \u001b[36m0.6931\u001b[0m  0.0853\n",
            "     28        0.6931  0.0743\n",
            "     29        0.6931  0.0832\n",
            "     30        0.6931  0.0928\n",
            "     31        0.6931  0.0839\n",
            "     32        0.6931  0.0724\n",
            "     33        0.6931  0.1091\n",
            "     34        0.6931  0.0736\n",
            "     35        0.6931  0.0773\n",
            "     36        0.6931  0.0757\n",
            "     37        0.6931  0.0978\n",
            "     38        0.6931  0.0855\n",
            "     39        0.6931  0.0773\n",
            "     40        0.6931  0.0907\n",
            "     41        0.6931  0.0773\n",
            "     42        0.6931  0.1052\n",
            "     43        0.6931  0.0964\n",
            "     44        0.6931  0.0804\n",
            "     45        0.6931  0.0930\n",
            "     46        0.6931  0.0811\n",
            "     47        0.6931  0.0780\n",
            "     48        0.6931  0.0790\n",
            "     49        0.6931  0.0767\n",
            "     50        0.6931  0.0795\n",
            "     51        0.6931  0.0773\n",
            "     52        0.6931  0.0722\n",
            "     53        0.6931  0.0871\n",
            "     54        0.6931  0.0845\n",
            "     55        0.6931  0.0870\n",
            "     56        0.6931  0.0840\n",
            "     57        0.6931  0.0911\n",
            "     58        0.6931  0.0923\n",
            "     59        \u001b[36m0.6930\u001b[0m  0.0730\n",
            "     60        \u001b[36m0.6919\u001b[0m  0.0939\n",
            "     61        0.6931  0.0921\n",
            "     62        0.6931  0.0864\n",
            "     63        0.6931  0.0911\n",
            "     64        0.6931  0.0900\n",
            "     65        0.6920  0.0959\n",
            "     66        0.6931  0.0912\n",
            "     67        \u001b[36m0.6918\u001b[0m  0.1105\n",
            "     68        0.6931  0.1450\n",
            "     69        0.6931  0.1851\n",
            "     70        0.6931  0.2044\n",
            "     71        0.6918  0.2036\n",
            "     72        \u001b[36m0.6905\u001b[0m  0.2205\n",
            "     73        0.6918  0.2249\n",
            "     74        0.6931  0.1708\n",
            "     75        \u001b[36m0.6904\u001b[0m  0.2974\n",
            "     76        0.6931  0.2545\n",
            "     77        0.6931  0.1457\n",
            "     78        0.6905  0.0764\n",
            "     79        0.6918  0.0806\n",
            "     80        \u001b[36m0.6869\u001b[0m  0.0787\n",
            "     81        \u001b[36m0.6801\u001b[0m  0.0823\n",
            "     82        0.6823  0.0986\n",
            "     83        0.6877  0.1372\n",
            "     84        0.6823  0.0892\n",
            "     85        0.6823  0.1160\n",
            "     86        \u001b[36m0.6796\u001b[0m  0.1071\n",
            "     87        0.6810  0.1112\n",
            "     88        0.6850  0.2475\n",
            "     89        0.6823  0.2008\n",
            "     90        0.6837  0.2205\n",
            "     91        0.6796  0.4603\n",
            "     92        0.6829  0.1651\n",
            "     93        0.6823  0.3044\n",
            "     94        0.6810  0.8277\n",
            "     95        0.6823  0.5487\n",
            "     96        0.6823  0.1476\n",
            "     97        0.6837  0.2718\n",
            "     98        0.6837  0.1093\n",
            "     99        \u001b[36m0.6769\u001b[0m  0.2448\n",
            "    100        0.6810  0.2846\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m29.2518\u001b[0m  0.3101\n",
            "      2       29.2721  0.1246\n",
            "      3       30.4196  0.1853\n",
            "      4       29.8188  0.1995\n",
            "      5       30.6177  0.2321\n",
            "      6       30.2203  0.1298\n",
            "      7       29.6370  0.0979\n",
            "      8       \u001b[36m29.2424\u001b[0m  0.3031\n",
            "      9       30.4020  0.2697\n",
            "     10       31.3718  0.3206\n",
            "     11       30.4142  0.2619\n",
            "     12       30.8049  0.1989\n",
            "     13       29.8175  0.0933\n",
            "     14       29.8229  0.0742\n",
            "     15       30.8008  0.1044\n",
            "     16       30.7859  0.0883\n",
            "     17       \u001b[36m28.8504\u001b[0m  0.1040\n",
            "     18       30.0195  0.0914\n",
            "     19       30.0114  0.1493\n",
            "     20       29.4228  0.0825\n",
            "     21       30.4129  0.0870\n",
            "     22       32.1558  0.1660\n",
            "     23       \u001b[36m15.6328\u001b[0m  0.1524\n",
            "     24        \u001b[36m0.6931\u001b[0m  0.1695\n",
            "     25        0.6931  0.1397\n",
            "     26        0.6931  0.1174\n",
            "     27        0.6931  0.0904\n",
            "     28        0.6931  0.0801\n",
            "     29        0.6931  0.0900\n",
            "     30        0.6931  0.0770\n",
            "     31        0.6931  0.3920\n",
            "     32        0.6931  0.3078\n",
            "     33        0.6931  0.4270\n",
            "     34        0.6931  0.1491\n",
            "     35        0.6931  0.3569\n",
            "     36        0.6931  0.4279\n",
            "     37        0.6931  0.1894\n",
            "     38        0.6931  0.1064\n",
            "     39        0.6931  0.0895\n",
            "     40        0.6931  0.0807\n",
            "     41        0.6931  0.1642\n",
            "     42        0.6931  0.1724\n",
            "     43        0.6931  0.1444\n",
            "     44        0.6931  0.1684\n",
            "     45        0.6931  0.1487\n",
            "     46        0.6931  0.1133\n",
            "     47        0.6931  0.0946\n",
            "     48        0.6931  0.0779\n",
            "     49        0.6931  0.0801\n",
            "     50        0.6931  0.0846\n",
            "     51        0.6931  0.0830\n",
            "     52        0.6931  0.0742\n",
            "     53        0.6931  0.0729\n",
            "     54        0.6931  0.1329\n",
            "     55        0.6931  0.1480\n",
            "     56        0.6931  0.1757\n",
            "     57        0.6931  0.0887\n",
            "     58        0.6931  0.0826\n",
            "     59        0.6931  0.1555\n",
            "     60        0.6931  0.1799\n",
            "     61        0.6931  0.1157\n",
            "     62        0.6931  0.0803\n",
            "     63        0.6931  0.2090\n",
            "     64        0.6931  0.1894\n",
            "     65        0.6931  0.0787\n",
            "     66        0.6931  0.0837\n",
            "     67        0.6931  0.1230\n",
            "     68        0.6931  0.1300\n",
            "     69        0.6931  0.0877\n",
            "     70        0.6931  0.0962\n",
            "     71        0.6931  0.1398\n",
            "     72        0.6931  0.2893\n",
            "     73        0.6931  0.1411\n",
            "     74        0.6931  0.1100\n",
            "     75        0.6935  0.2068\n",
            "     76        0.6931  0.1112\n",
            "     77        0.6931  0.1634\n",
            "     78        0.6931  0.1364\n",
            "     79        0.6931  0.3119\n",
            "     80        0.6931  0.1257\n",
            "     81        0.6931  0.2528\n",
            "     82        0.6931  0.1153\n",
            "     83        0.6931  0.1250\n",
            "     84        0.6931  0.1822\n",
            "     85        0.6931  0.1838\n",
            "     86        0.6931  0.2933\n",
            "     87        0.6931  0.2439\n",
            "     88        0.6931  0.1243\n",
            "     89        0.6931  0.1862\n",
            "     90        0.6931  0.1982\n",
            "     91        0.6931  0.1791\n",
            "     92        0.6931  0.0938\n",
            "     93        0.6931  0.1499\n",
            "     94        0.6931  0.1468\n",
            "     95        0.6931  0.1344\n",
            "     96        0.6931  0.0853\n",
            "     97        0.6931  0.0856\n",
            "     98        0.6931  0.1320\n",
            "     99        0.6931  0.1418\n",
            "    100        0.6931  0.1627\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m30.7710\u001b[0m  0.1420\n",
            "      2       \u001b[36m30.6150\u001b[0m  0.1494\n",
            "      3       \u001b[36m28.8720\u001b[0m  0.1921\n",
            "      4       30.8130  0.4330\n",
            "      5       \u001b[36m28.6727\u001b[0m  0.2446\n",
            "      6       30.0250  0.0826\n",
            "      7       29.0525  0.0777\n",
            "      8       \u001b[36m28.0908\u001b[0m  0.1033\n",
            "      9       32.7323  0.0715\n",
            "     10       28.0949  0.0786\n",
            "     11       \u001b[36m27.4913\u001b[0m  0.0751\n",
            "     12       31.7828  0.1395\n",
            "     13       30.2149  0.0738\n",
            "     14       28.6605  0.0921\n",
            "     15       29.8337  0.0825\n",
            "     16       29.6208  0.0895\n",
            "     17       28.2699  0.0874\n",
            "     18       30.8035  0.1433\n",
            "     19       29.4417  0.0962\n",
            "     20       31.7719  0.0836\n",
            "     21       30.0209  0.0834\n",
            "     22       29.8161  0.0720\n",
            "     23       30.4088  0.0915\n",
            "     24       29.0768  0.1029\n",
            "     25       31.3759  0.0754\n",
            "     26       28.2604  0.0840\n",
            "     27       29.2723  0.1235\n",
            "     28       \u001b[36m10.7067\u001b[0m  0.2057\n",
            "     29        \u001b[36m0.6931\u001b[0m  0.1545\n",
            "     30        0.6931  0.1384\n",
            "     31        0.6931  0.2039\n",
            "     32        0.6931  0.4672\n",
            "     33        0.6931  0.3224\n",
            "     34        0.6931  0.3819\n",
            "     35        0.6931  0.2333\n",
            "     36        0.6931  0.1880\n",
            "     37        0.6931  0.2478\n",
            "     38        0.6931  0.2666\n",
            "     39        0.6931  0.1476\n",
            "     40        0.6931  0.2465\n",
            "     41        0.6931  0.2170\n",
            "     42        0.6931  0.1932\n",
            "     43        0.6931  0.1630\n",
            "     44        0.6931  0.0710\n",
            "     45        0.6931  0.0761\n",
            "     46        0.6931  0.1718\n",
            "     47        0.6931  0.2146\n",
            "     48        0.6931  0.2365\n",
            "     49        0.6931  0.1662\n",
            "     50        0.6931  0.1617\n",
            "     51        0.6931  0.1116\n",
            "     52        0.6931  0.0746\n",
            "     53        0.6931  0.0990\n",
            "     54        0.6931  0.1869\n",
            "     55        0.6931  0.3669\n",
            "     56        0.6931  0.4027\n",
            "     57        0.6931  0.3187\n",
            "     58        0.6931  0.1756\n",
            "     59        0.6931  0.1138\n",
            "     60        0.6931  0.1182\n",
            "     61        0.6931  0.1105\n",
            "     62        0.6931  0.1241\n",
            "     63        0.6931  0.1126\n",
            "     64        0.6931  0.1078\n",
            "     65        0.6931  0.0990\n",
            "     66        0.6931  0.1089\n",
            "     67        0.6931  0.1142\n",
            "     68        0.6931  0.1097\n",
            "     69        0.6931  0.1186\n",
            "     70        0.6931  0.1081\n",
            "     71        0.6931  0.1173\n",
            "     72        0.6931  0.1034\n",
            "     73        0.6931  0.1249\n",
            "     74        0.6931  0.1143\n",
            "     75        0.6931  0.1314\n",
            "     76        0.6931  0.1316\n",
            "     77        0.6931  0.1354\n",
            "     78        0.6931  0.0801\n",
            "     79        0.6931  0.0887\n",
            "     80        0.6931  0.0727\n",
            "     81        0.6931  0.0772\n",
            "     82        0.6931  0.0749\n",
            "     83        0.6931  0.0927\n",
            "     84        0.6931  0.1297\n",
            "     85        0.6931  0.1851\n",
            "     86        0.6931  0.1046\n",
            "     87        0.6931  0.0966\n",
            "     88        0.6931  0.0931\n",
            "     89        0.6931  0.1785\n",
            "     90        0.6931  0.0902\n",
            "     91        0.6931  0.0797\n",
            "     92        0.6931  0.1445\n",
            "     93        0.6931  0.0892\n",
            "     94        0.6931  0.0818\n",
            "     95        0.6931  0.0909\n",
            "     96        0.6931  0.2049\n",
            "     97        0.6931  0.1426\n",
            "     98        0.6931  0.1104\n",
            "     99        0.6931  0.0758\n",
            "    100        0.6931  0.0784\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m30.2000\u001b[0m  0.1331\n",
            "      2       \u001b[36m29.0389\u001b[0m  0.1016\n",
            "      3       29.2437  0.0752\n",
            "      4       \u001b[36m28.0800\u001b[0m  0.0830\n",
            "      5       28.8355  0.1952\n",
            "      6       28.6686  0.2324\n",
            "      7       28.8328  0.1007\n",
            "      8       29.8242  0.0845\n",
            "      9       30.9799  0.1161\n",
            "     10       30.2094  0.1695\n",
            "     11       30.4020  0.0951\n",
            "     12       32.7323  0.0761\n",
            "     13       30.4305  0.1571\n",
            "     14       30.9920  0.1100\n",
            "     15       29.8269  0.0796\n",
            "     16       31.3651  0.1274\n",
            "     17       \u001b[36m28.0651\u001b[0m  0.2122\n",
            "     18       30.3912  0.1795\n",
            "     19       30.0317  0.1840\n",
            "     20       30.7967  0.2397\n",
            "     21       31.7571  0.2552\n",
            "     22       28.6564  0.1822\n",
            "     23       29.4241  0.1498\n",
            "     24       \u001b[36m12.2829\u001b[0m  0.1451\n",
            "     25        \u001b[36m0.6931\u001b[0m  0.0788\n",
            "     26        \u001b[36m0.6919\u001b[0m  0.0737\n",
            "     27        \u001b[36m0.6877\u001b[0m  0.0802\n",
            "     28        \u001b[36m0.6689\u001b[0m  0.0900\n",
            "     29        \u001b[36m0.6309\u001b[0m  0.0733\n",
            "     30        0.8540  0.0879\n",
            "     31        0.6384  0.1031\n",
            "     32        \u001b[36m0.6265\u001b[0m  0.0736\n",
            "     33        0.6309  0.0866\n",
            "     34        \u001b[36m0.5960\u001b[0m  0.0811\n",
            "     35        0.6067  0.0697\n",
            "     36        0.6262  0.0709\n",
            "     37        0.6047  0.0734\n",
            "     38        \u001b[36m0.5882\u001b[0m  0.0841\n",
            "     39        \u001b[36m0.5881\u001b[0m  0.0788\n",
            "     40        \u001b[36m0.5870\u001b[0m  0.0754\n",
            "     41        0.5886  0.0839\n",
            "     42        0.5926  0.0844\n",
            "     43        \u001b[36m0.5826\u001b[0m  0.0785\n",
            "     44        \u001b[36m0.5764\u001b[0m  0.0878\n",
            "     45        0.5775  0.0933\n",
            "     46        0.5783  0.0800\n",
            "     47        0.5806  0.0729\n",
            "     48        0.5893  0.0751\n",
            "     49        0.6070  0.0858\n",
            "     50        0.5887  0.0840\n",
            "     51        0.5797  0.0789\n",
            "     52        0.5851  0.0826\n",
            "     53        0.5958  0.0794\n",
            "     54        \u001b[36m0.5580\u001b[0m  0.0751\n",
            "     55        0.5647  0.0758\n",
            "     56        0.5781  0.0888\n",
            "     57        0.5727  0.0827\n",
            "     58        0.5587  0.0849\n",
            "     59        0.5733  0.0756\n",
            "     60        0.5665  0.0777\n",
            "     61        0.5750  0.0786\n",
            "     62        0.5752  0.0754\n",
            "     63        0.5704  0.0974\n",
            "     64        0.5760  0.0839\n",
            "     65        0.5638  0.0986\n",
            "     66        0.5647  0.0854\n",
            "     67        0.5697  0.0796\n",
            "     68        0.5702  0.0955\n",
            "     69        0.5738  0.0800\n",
            "     70        0.5738  0.1343\n",
            "     71        0.5650  0.1378\n",
            "     72        \u001b[36m0.5559\u001b[0m  0.1418\n",
            "     73        \u001b[36m0.5387\u001b[0m  0.1473\n",
            "     74        0.5742  0.3703\n",
            "     75        0.5709  0.3588\n",
            "     76        0.5669  0.2914\n",
            "     77        0.5757  0.3496\n",
            "     78        0.5448  0.1172\n",
            "     79        0.5602  0.1141\n",
            "     80        0.5511  0.1066\n",
            "     81        0.5707  0.1185\n",
            "     82        0.5486  0.1197\n",
            "     83        0.5585  0.1326\n",
            "     84        0.5643  0.1253\n",
            "     85        0.5718  0.1245\n",
            "     86        0.5572  0.1219\n",
            "     87        0.5499  0.1204\n",
            "     88        0.5404  0.1138\n",
            "     89        0.5468  0.0951\n",
            "     90        0.5513  0.0835\n",
            "     91        0.5453  0.0813\n",
            "     92        0.5528  0.0769\n",
            "     93        0.5588  0.0975\n",
            "     94        0.5569  0.0828\n",
            "     95        0.5646  0.0809\n",
            "     96        0.5497  0.0827\n",
            "     97        \u001b[36m0.5354\u001b[0m  0.0939\n",
            "     98        0.5430  0.0866\n",
            "     99        0.5391  0.0802\n",
            "    100        0.5502  0.0841\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m30.9758\u001b[0m  0.0888\n",
            "      2       \u001b[36m30.6068\u001b[0m  0.0889\n",
            "      3       \u001b[36m29.8378\u001b[0m  0.0859\n",
            "      4       30.2203  0.0925\n",
            "      5       30.9948  0.0968\n",
            "      6       \u001b[36m27.4967\u001b[0m  0.0963\n",
            "      7       28.6713  0.0805\n",
            "      8       30.9785  0.0853\n",
            "      9       30.6001  0.0931\n",
            "     10       29.4458  0.0838\n",
            "     11       30.2270  0.0793\n",
            "     12       29.0335  0.0749\n",
            "     13       28.2712  0.0806\n",
            "     14       29.4295  0.0758\n",
            "     15       30.2203  0.0812\n",
            "     16       29.6465  0.0767\n",
            "     17       27.9022  0.0838\n",
            "     18       32.3335  0.0772\n",
            "     19       30.7981  0.0782\n",
            "     20       30.3939  0.0906\n",
            "     21       29.2315  0.0759\n",
            "     22       27.8860  0.0744\n",
            "     23       \u001b[36m22.0129\u001b[0m  0.0846\n",
            "     24        \u001b[36m0.6931\u001b[0m  0.0773\n",
            "     25        \u001b[36m0.6905\u001b[0m  0.0757\n",
            "     26        0.6931  0.0750\n",
            "     27        0.6935  0.0745\n",
            "     28        0.6931  0.1021\n",
            "     29        0.6931  0.0892\n",
            "     30        0.6931  0.0871\n",
            "     31        0.6931  0.0733\n",
            "     32        0.6931  0.0908\n",
            "     33        0.6931  0.0923\n",
            "     34        0.6931  0.0930\n",
            "     35        0.6931  0.0925\n",
            "     36        0.6931  0.0830\n",
            "     37        0.6931  0.0952\n",
            "     38        0.6931  0.0858\n",
            "     39        0.6931  0.0800\n",
            "     40        0.6931  0.0792\n",
            "     41        0.6918  0.0840\n",
            "     42        0.6931  0.0734\n",
            "     43        0.6931  0.0778\n",
            "     44        0.6921  0.0863\n",
            "     45        0.6931  0.0856\n",
            "     46        0.6931  0.0765\n",
            "     47        0.6931  0.0773\n",
            "     48        0.6931  0.0955\n",
            "     49        0.6931  0.0780\n",
            "     50        0.6931  0.0773\n",
            "     51        0.6931  0.0811\n",
            "     52        0.6931  0.0781\n",
            "     53        0.6931  0.0718\n",
            "     54        0.6918  0.0815\n",
            "     55        0.6931  0.0822\n",
            "     56        0.6931  0.0901\n",
            "     57        0.6931  0.0742\n",
            "     58        0.6919  0.0900\n",
            "     59        0.6954  0.0811\n",
            "     60        0.8988  0.0824\n",
            "     61        \u001b[36m0.6669\u001b[0m  0.0750\n",
            "     62        0.8961  0.0796\n",
            "     63        0.6741  0.0801\n",
            "     64        \u001b[36m0.6651\u001b[0m  0.0768\n",
            "     65        0.7034  0.0927\n",
            "     66        0.6891  0.0857\n",
            "     67        0.6892  0.0796\n",
            "     68        0.6813  0.0797\n",
            "     69        \u001b[36m0.6579\u001b[0m  0.0825\n",
            "     70        \u001b[36m0.6500\u001b[0m  0.0814\n",
            "     71        0.6607  0.0818\n",
            "     72        0.6594  0.0752\n",
            "     73        \u001b[36m0.6471\u001b[0m  0.0809\n",
            "     74        0.6549  0.0791\n",
            "     75        0.6579  0.0828\n",
            "     76        0.6552  0.0868\n",
            "     77        0.6539  0.0758\n",
            "     78        0.6579  0.0834\n",
            "     79        0.6634  0.0747\n",
            "     80        0.6607  0.0779\n",
            "     81        0.6607  0.1224\n",
            "     82        0.6552  0.0739\n",
            "     83        0.6539  0.0876\n",
            "     84        0.6647  0.0960\n",
            "     85        0.6517  0.0870\n",
            "     86        0.6525  0.0952\n",
            "     87        \u001b[36m0.6458\u001b[0m  0.0852\n",
            "     88        0.6512  0.1047\n",
            "     89        0.6475  0.0875\n",
            "     90        0.6777  0.1001\n",
            "     91        0.6552  0.1061\n",
            "     92        0.6607  0.0923\n",
            "     93        0.6525  0.0919\n",
            "     94        0.6512  0.0935\n",
            "     95        0.6539  0.0865\n",
            "     96        0.6607  0.1058\n",
            "     97        0.6552  0.0951\n",
            "     98        0.6579  0.0869\n",
            "     99        0.6525  0.1007\n",
            "    100        0.6553  0.1111\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m27.2401\u001b[0m  0.1067\n",
            "      2       29.3803  0.1551\n",
            "      3       31.3093  0.1652\n",
            "      4       30.5350  0.1319\n",
            "      5       31.3228  0.1063\n",
            "      6       30.1397  0.1170\n",
            "      7       28.6046  0.1214\n",
            "      8       31.9022  0.1156\n",
            "      9       30.5350  0.1203\n",
            "     10       29.7810  0.1197\n",
            "     11       29.3924  0.1140\n",
            "     12       29.5820  0.1253\n",
            "     13       28.6127  0.1247\n",
            "     14       31.3174  0.1173\n",
            "     15       29.9664  0.1379\n",
            "     16       30.9424  0.1278\n",
            "     17       28.2310  0.1852\n",
            "     18       31.1239  0.1421\n",
            "     19       28.9891  0.1421\n",
            "     20       30.1343  0.1395\n",
            "     21       30.5431  0.1382\n",
            "     22       29.9732  0.1447\n",
            "     23       33.0705  0.1522\n",
            "     24       31.7225  0.1045\n",
            "     25        \u001b[36m5.8066\u001b[0m  0.1686\n",
            "     26        \u001b[36m0.6609\u001b[0m  0.1189\n",
            "     27        \u001b[36m0.6580\u001b[0m  0.0734\n",
            "     28        0.6634  0.0857\n",
            "     29        0.6607  0.0801\n",
            "     30        0.6634  0.0831\n",
            "     31        \u001b[36m0.6575\u001b[0m  0.0783\n",
            "     32        0.6607  0.0805\n",
            "     33        0.6648  0.0774\n",
            "     34        0.6688  0.0773\n",
            "     35        0.6648  0.0816\n",
            "     36        0.6634  0.0928\n",
            "     37        0.6621  0.0756\n",
            "     38        \u001b[36m0.6553\u001b[0m  0.0823\n",
            "     39        0.6607  0.0811\n",
            "     40        0.6634  0.0761\n",
            "     41        0.6621  0.0783\n",
            "     42        0.6607  0.0761\n",
            "     43        0.6557  0.0758\n",
            "     44        \u001b[36m0.6486\u001b[0m  0.0824\n",
            "     45        0.6553  0.0742\n",
            "     46        \u001b[36m0.6486\u001b[0m  0.0764\n",
            "     47        \u001b[36m0.6432\u001b[0m  0.0860\n",
            "     48        \u001b[36m0.6418\u001b[0m  0.0886\n",
            "     49        0.6459  0.0786\n",
            "     50        0.6545  0.0854\n",
            "     51        0.6513  0.0962\n",
            "     52        0.6432  0.0812\n",
            "     53        0.6486  0.0754\n",
            "     54        0.6486  0.0834\n",
            "     55        0.6499  0.0743\n",
            "     56        0.6459  0.0752\n",
            "     57        0.6580  0.0754\n",
            "     58        0.6432  0.0773\n",
            "     59        0.6540  0.0718\n",
            "     60        0.6513  0.0864\n",
            "     61        0.6507  0.0901\n",
            "     62        0.6472  0.0811\n",
            "     63        0.6526  0.0888\n",
            "     64        0.6486  0.0843\n",
            "     65        0.6445  0.0731\n",
            "     66        0.6580  0.0831\n",
            "     67        0.6486  0.0808\n",
            "     68        0.6459  0.0824\n",
            "     69        0.6513  0.0966\n",
            "     70        0.6472  0.0858\n",
            "     71        0.6526  0.0840\n",
            "     72        0.6553  0.0996\n",
            "     73        \u001b[36m0.6405\u001b[0m  0.0992\n",
            "     74        0.6513  0.0908\n",
            "     75        0.6540  0.1042\n",
            "     76        0.6459  0.1038\n",
            "     77        0.6446  0.0935\n",
            "     78        0.6486  0.1076\n",
            "     79        0.6472  0.1265\n",
            "     80        0.6526  0.1175\n",
            "     81        0.6445  0.0882\n",
            "     82        0.6418  0.0998\n",
            "     83        0.6526  0.0861\n",
            "     84        0.6553  0.0927\n",
            "     85        0.6540  0.0827\n",
            "     86        0.6472  0.0728\n",
            "     87        0.6480  0.0719\n",
            "     88        0.6526  0.0856\n",
            "     89        0.6459  0.0784\n",
            "     90        0.6531  0.0825\n",
            "     91        0.6486  0.0772\n",
            "     92        0.6472  0.0752\n",
            "     93        0.6472  0.0750\n",
            "     94        0.6459  0.1014\n",
            "     95        0.6472  0.0821\n",
            "     96        0.6510  0.0864\n",
            "     97        0.6486  0.0867\n",
            "     98        0.6464  0.0892\n",
            "     99        \u001b[36m0.6243\u001b[0m  0.0807\n",
            "    100        0.6557  0.0882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "media = resultados.mean()\n",
        "desvio_padrao = resultados.std()\n",
        "media, desvio_padrao"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o34zq6xWvwCO",
        "outputId": "379d99e4-f83b-48ea-ad0f-cf20cf173af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4712092731829574, 0.13489615886436063)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultados"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWT1_ajCwOld",
        "outputId": "1e689c15-dc69-48b6-9a82-1d4c62e284a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.38596491, 0.38596491, 0.63157895, 0.36842105, 0.40350877,\n",
              "       0.36842105, 0.36842105, 0.77192982, 0.43859649, 0.58928571])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}